{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "452aa5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a61b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dir = os.path.join('Datasets', 'ASL-static', 'archive', 'train_all')\n",
    "# for sign in os.listdir(train_directory):\n",
    "#     for image in os.listdir(os.path.join(train_directory, sign)):\n",
    "#         shutil.copy(os.path.join(train_directory, sign, image), os.path.join(all_dir, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84751b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join('Datasets', 'ASL-static', 'archive', 'asl_alphabet_train')\n",
    "train_all_dir = os.path.join('Datasets', 'ASL-static', 'archive', 'train_all')\n",
    "test_dir = os.path.join('Datasets', 'ASL-static', 'archive', 'asl_alphabet_test')\n",
    "\n",
    "sign_to_num = {l: i for i, l in enumerate(os.listdir(train_dir))}\n",
    "num_to_sign = {i: l for l, i in sign_to_num.items()} \n",
    "one_hots = {sign: one_hot for sign, one_hot in zip(sign_to_num.keys(), F.one_hot(torch.tensor(list(sign_to_num.values())), num_classes=len(sign_to_num)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7485224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = len(sign_to_num)\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35c6df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Pranav/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31547c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandSignsDataset(Dataset):\n",
    "    def __init__(self, directory, sign_to_num, preprocess=None):\n",
    "        self.directory = directory\n",
    "        self.img_files = os.listdir(directory)\n",
    "        self.preprocess = preprocess\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.img_files[index]\n",
    "        img_path = os.path.join(self.directory, img_name)\n",
    "        img_class = \"\".join([i for i in img_name if not i.isdigit()])[:-4]\n",
    "        image = Image.open(img_path)\n",
    "        \n",
    "        if self.preprocess:\n",
    "            image = self.preprocess(image)\n",
    "        \n",
    "        return (image, sign_to_num[img_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4031866",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = HandSignsDataset(\n",
    "    directory = train_all_dir,\n",
    "    sign_to_num = one_hots,\n",
    "    preprocess = preprocess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74db9e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85260, 10000, 870, 870)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(0.98 * len(dataset))\n",
    "small_train_size = 10000\n",
    "val_size = int(len(dataset) - train_size)//2\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_size, small_train_size, val_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71020b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "small_train_set, _ = torch.utils.data.random_split(train_set, [small_train_size, train_size-small_train_size])\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "small_train_loader = DataLoader(dataset=small_train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc30245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = torch.nn.Linear(2048, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "009883c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a053f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "beb9d5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, n_epochs, small=False):\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    test_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        train_progress = tqdm(range(small_train_size if small else train_size))\n",
    "        i = 0\n",
    "        for inputs, labels in dataloaders['train']:\n",
    "#             if i%10 == 0:\n",
    "#                 train_error = test_model(model, dataloaders['train'], criterion)\n",
    "#                 val_error = test_model(model, dataloaders['val'], criterion)\n",
    "#                 test_error = test_model(model, dataloaders['test'], criterion)\n",
    "#                 print(train_error, val_error, test_error)\n",
    "#                 train_history.append(train_error)\n",
    "#                 val_history.append(val_error)\n",
    "#                 test_history.append(test_error)\n",
    "#                 model.train()\n",
    "                \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs, aux_outputs = model(inputs)\n",
    "            loss1 = criterion(outputs, labels)\n",
    "            loss2 = criterion(aux_outputs, labels)\n",
    "            loss = loss1 + 0.4*loss2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print(f\"Output shape: {outputs.shape}, Labels shape: {labels.shape}\")\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            train_progress.update(len(inputs))\n",
    "\n",
    "            del inputs\n",
    "            del labels\n",
    "            del outputs\n",
    "            del aux_outputs\n",
    "            del loss\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "    return train_history, val_history, test_history\n",
    "            \n",
    "def test_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    length = 0\n",
    "    corrects = 0\n",
    "    test_progress = tqdm(range(len(loader)))\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=-1)\n",
    "        corrects += (preds==labels).sum()\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        length += inputs.size(0)\n",
    "        test_progress.update(1)\n",
    "        \n",
    "        del inputs\n",
    "        del labels\n",
    "        del outputs\n",
    "        del loss\n",
    "        \n",
    "    return running_loss/length, corrects/length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a12d1b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895d3d00094542e6b917d719b5a08788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.04653847160935402, tensor(0.9883, device='cuda:0'))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, small_train_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f567866",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636b50e58be64a2a86963831a8674f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.07585995257243343, tensor(0.9759, device='cuda:0'))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9e01398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262b8be3ea6f440baaff548fabc5eeca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.04771220006434054, tensor(0.9874, device='cuda:0'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39bf031e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1310e1b3e846b789df8154c55f72bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf901054660440d28f5b10536fc5003d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "histories = train_model(\n",
    "    model = model,\n",
    "    dataloaders = {'train': small_train_loader, 'val': val_loader, 'test': test_loader},\n",
    "    criterion = criterion,\n",
    "    optimizer = optimizer,\n",
    "    n_epochs = 2,\n",
    "    small = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5efe15b0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x129f5e1cc70>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(histories[0], label='train')\n",
    "plt.plot(histories[1], label='val')\n",
    "plt.plot(histories[2], label='test')\n",
    "plt.xlabel('Batches trained / 7')\n",
    "plt.xlabel('Loss')\n",
    "plt.title('Loss history')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17a018bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df54d7b995c74cb9a7fee776af4c0c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.07584635515633072"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4079b92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_file_name = os.listdir(test_dir)[-1]\n",
    "example = Image.open(os.path.join(test_dir, example_file_name))\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463cae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_file_name.split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b5b5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "inp = preprocess(example).to(device)\n",
    "outs = model(inp.unsqueeze(0))\n",
    "num_to_sign[torch.argmax(outs).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14831a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual\t\tPredicted\tConfidence\tCorrect\n",
      "A\t\tA\t\t99.97%\t\t✅\n",
      "B\t\tB\t\t99.68%\t\t✅\n",
      "C\t\tC\t\t100.00%\t\t✅\n",
      "D\t\tD\t\t100.00%\t\t✅\n",
      "E\t\tE\t\t92.29%\t\t✅\n",
      "F\t\tF\t\t100.00%\t\t✅\n",
      "G\t\tG\t\t100.00%\t\t✅\n",
      "H\t\tH\t\t99.98%\t\t✅\n",
      "I\t\tI\t\t100.00%\t\t✅\n",
      "J\t\tJ\t\t100.00%\t\t✅\n",
      "K\t\tK\t\t99.98%\t\t✅\n",
      "L\t\tL\t\t99.96%\t\t✅\n",
      "M\t\tM\t\t99.89%\t\t✅\n",
      "nothing\t\tnothing\t\t99.38%\t\t✅\n",
      "N\t\tN\t\t94.05%\t\t✅\n",
      "O\t\tO\t\t98.84%\t\t✅\n",
      "P\t\tP\t\t100.00%\t\t✅\n",
      "Q\t\tQ\t\t99.69%\t\t✅\n",
      "R\t\tR\t\t100.00%\t\t✅\n",
      "space\t\tspace\t\t100.00%\t\t✅\n",
      "S\t\tS\t\t99.12%\t\t✅\n",
      "T\t\tT\t\t99.92%\t\t✅\n",
      "U\t\tU\t\t99.97%\t\t✅\n",
      "V\t\tV\t\t99.98%\t\t✅\n",
      "W\t\tW\t\t100.00%\t\t✅\n",
      "X\t\tX\t\t100.00%\t\t✅\n",
      "Y\t\tY\t\t99.83%\t\t✅\n",
      "Z\t\tZ\t\t87.06%\t\t✅\n",
      "\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(\"Actual\\t\\tPredicted\\tConfidence\\tCorrect\")\n",
    "corrects = wrongs = 0\n",
    "for f in os.listdir(test_dir):\n",
    "    test = Image.open(os.path.join(test_dir, f))\n",
    "    actual_label = f.split(\"_\")[0]\n",
    "    inp = preprocess(test).to(device)\n",
    "    outs = model(inp.unsqueeze(0))\n",
    "    confidence = torch.max(torch.nn.Softmax(dim=1)(outs)).item()\n",
    "    predicted_label = num_to_sign[torch.argmax(outs).item()]\n",
    "    \n",
    "    if actual_label == predicted_label:\n",
    "        print(\"{a}\\t\\t{p}\\t\\t{c:.2%}\\t\\t✅\".format(a=actual_label, p=predicted_label, c=confidence))\n",
    "        corrects += 1\n",
    "    else:\n",
    "        print(\"{a}\\t\\t{p}\\t\\t{c:.2%}\\t\\t❌\".format(a=actual_label, p=predicted_label, c=confidence))\n",
    "        wrongs += 1\n",
    "        \n",
    "print(\"\\nAccuracy: {:.2%}\".format(corrects/(corrects+wrongs)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f10cc0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
