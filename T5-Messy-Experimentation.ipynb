{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a54b8b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "#import torchvideo.transforms as VT\n",
    "import torchvision.transforms as IT\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "from transformers import BertConfig, BertForSequenceClassification, EncoderDecoderModel, BertModel, AutoConfig\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, BertTokenizer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import get_scheduler\n",
    "from torchviz import make_dot\n",
    "from datasets import list_datasets, load_metric\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8d81b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = r\"C:\\Sign-Language-Recognition\\Datasets\\PHOENIX14T_small\\archive\\videos_phoenix\\videos\\train\"\n",
    "files = os.listdir(train_dir)\n",
    "\n",
    "im_size = 224\n",
    "vid_batch_size = 8\n",
    "frame_batch_size = 8\n",
    "slrt_input_len = 200\n",
    "max_vid_len = 200\n",
    "frame_encoding_size = 1024\n",
    "recog_loss_weight = 0.01\n",
    "max_gradient_norm = 5\n",
    "\n",
    "assert((vid_batch_size * max_vid_len) % frame_batch_size == 0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f418ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths = []\n",
    "# for vid in tqdm(files):\n",
    "#     cap = cv2.VideoCapture(os.path.join(train_dir, vid))\n",
    "#     length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     lengths.append(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9678cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86411b72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vid = torchvision.io.read_video(os.path.join(train_dir, files[0])) # vid[0] = time x height x width x channels\n",
    "print(vid[0].shape)\n",
    "vid = F.interpolate(vid[0].permute(0, 3, 1, 2), size=299) # vid = time x channels x height x width\n",
    "IT.ToPILImage()(vid[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d7b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoenixDataset(Dataset):\n",
    "    def __init__(self, annotations_path):\n",
    "        \n",
    "        with gzip.open(annotations_path, 'rb') as f:\n",
    "            self.annotations = pickle.load(f)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        #vid = torchvision.io.read_video(os.path.join(self.vid_dir, self.vid_files[index]))[0] # vid = time x height x width x channels\n",
    "        #vid = vid[:max_vid_len]\n",
    "        #vid_len = vid.shape[0]\n",
    "        #vid = F.interpolate(vid.permute(0, 3, 1, 2), size=self.im_size)\n",
    "        #vid = IT.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(vid.type(torch.FloatTensor))\n",
    "        #vid = torch.cat((vid, torch.zeros(max_vid_len - vid.shape[0], *vid.shape[1:])), dim=0)\n",
    "        sign, gloss, text = self.annotations[index]['sign'], self.annotations[index]['gloss'], self.annotations[index]['text']\n",
    "        sign = sign[:slrt_input_len]\n",
    "        sign = torch.cat([sign, torch.zeros(slrt_input_len - sign.shape[0], *sign.shape[1:])], dim=0)\n",
    "\n",
    "        return (sign, gloss, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "863af6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "phoenix_train = PhoenixDataset(\n",
    "    annotations_path = r\"C:\\Sign-Language-Recognition\\Datasets\\PHOENIX14T\\phoenix14t.pami0.train\",\n",
    ")\n",
    "\n",
    "indices = np.random.choice(len(phoenix_train), 500, replace=False)\n",
    "phoenix_train_small = Subset(phoenix_train, indices)\n",
    "\n",
    "phoenix_val = PhoenixDataset(\n",
    "    annotations_path = r\"C:\\Sign-Language-Recognition\\Datasets\\PHOENIX14T\\phoenix14t.pami0.dev\",\n",
    ")\n",
    "\n",
    "phoenix_test = PhoenixDataset(\n",
    "    annotations_path = r\"C:\\Sign-Language-Recognition\\Datasets\\PHOENIX14T\\phoenix14t.pami0.test\",\n",
    ")\n",
    "\n",
    "phoenix_train_loader = DataLoader(dataset=phoenix_train, batch_size=vid_batch_size, shuffle=True, num_workers=0)\n",
    "phonix_train_small_loader = DataLoader(dataset=phoenix_train_small, batch_size=vid_batch_size, shuffle=True, num_workers=0)\n",
    "phoenix_val_loader = DataLoader(dataset=phoenix_val, batch_size=vid_batch_size, shuffle=True, num_workers=0)\n",
    "phoenix_test_loader = DataLoader(dataset=phoenix_test, batch_size=vid_batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1ca1cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7096, 500, 519, 642)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phoenix_train), len(phoenix_train_small), len(phoenix_val), len(phoenix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123523fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phoenix_train[3][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97c2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# frame_encoder = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\n",
    "# for i, (n, p) in enumerate(frame_encoder.named_parameters()):\n",
    "#     if i > 150:\n",
    "#         break\n",
    "#     p.requires_grad_(False)\n",
    "# frame_encoder.classifier = nn.Sequential(nn.Flatten(), nn.Linear(62720, frame_encoding_size))\n",
    "# cnn_optimizer = torch.optim.AdamW(frame_encoder.parameters(), lr=1e-3)\n",
    "# frame_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0890aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce61129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len([n for n, p in frame_encoder.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f29b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(iter(phoenix_train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0873d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_v_len = torch.max(batch[0])\n",
    "# vid_encoder_input = torch.empty(vid_batch_size, batch_v_len, frame_encoding_size, device=device)\n",
    "# for vb in tqdm(range(vid_batch_size)):\n",
    "#     v_len = batch[0][vb].item()\n",
    "#     single_vid_encoding = torch.empty(v_len, frame_encoding_size, device=device)\n",
    "#     for i in range(math.ceil(v_len/frame_batch_size)):\n",
    "#         #print(\"1: {:,}\".format(torch.cuda.memory_allocated(0)))\n",
    "#         start_frame = i * frame_batch_size\n",
    "#         #print(\"2: {:,}\".format(torch.cuda.memory_allocated(0)))\n",
    "#         end_frame = min((i + 1) * frame_batch_size, v_len)\n",
    "#         print(\"3: {:,}\".format(torch.cuda.memory_allocated(0)))\n",
    "#         fe_input = batch[1][vb][start_frame:end_frame].to(device)\n",
    "#         #print(\"4: {:,}\".format(torch.cuda.memory_allocated(0)))\n",
    "#         encoded_frames = frame_encoder(fe_input)\n",
    "#         #print(\"5: {:,}\".format(torch.cuda.memory_allocated(0)))\n",
    "#         single_vid_encoding[start_frame:end_frame, :] = encoded_frames.unsqueeze(0)\n",
    "#         #print(\"6: {:,}\".format(torch.cuda.memory_allocated(0)))\n",
    "#         del fe_input\n",
    "#         torch.cuda.empty_cache()\n",
    "#         #print(\"7: {:,}\".format(torch.cuda.memory_allocated(0)))\n",
    "#         del encoded_frames\n",
    "#         torch.cuda.empty_cache()\n",
    "#         #print(\"8: {:,}\".format(torch.cuda.memory_allocated(0)))\n",
    "#     single_vid_encoding = torch.cat((single_vid_encoding, torch.zeros(batch_v_len - v_len, frame_encoding_size, device=device)), dim = 0)\n",
    "#     vid_encoder_input[vb, :, :] = single_vid_encoding\n",
    "#     del single_vid_encoding\n",
    "# vid_encoder_input = torch.cat((vid_encoder_input, torch.zeros(vid_batch_size, slrt_input_len - vid_encoder_input.shape[1], frame_encoding_size, device=device)), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277b607",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vid_encoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae03f71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make_dot(vid_encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dfa9d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "german_tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/german-gpt2\")\n",
    "german_gpt = AutoModelWithLMHead.from_pretrained(\"dbmdz/german-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93090ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(german_tokenizer)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = german_tokenizer(\"MORGEN TEMPERATUR ELF SAUER LAND BIS MAXIMAL EINS ZWANZIG BERG OST\", return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c145bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_tokenizer.decode(encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1106428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ebd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_config = BertConfig(\n",
    "#     hidden_size = frame_encoding_size,\n",
    "#     num_hidden_layers = 1,\n",
    "#     num_attention_heads = 8,\n",
    "#     intermetiate_size = frame_encoding_size * 2,\n",
    "#     max_length = slrt_input_len\n",
    "# )\n",
    "# bert = BertModel(bert_config, add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298bf3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d19267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_dot(encoder(test_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b92b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4abe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_gpt.transformer.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f3de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(frame_encoding_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "                            d_model = frame_encoding_size,\n",
    "                            nhead = 8,\n",
    "                            dim_feedforward = frame_encoding_size * 2,\n",
    "                            batch_first = True\n",
    "                        )\n",
    "        self.layer_norm = nn.LayerNorm(frame_encoding_size)\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        x = self.positional_encoding(embeddings)\n",
    "        x = self.encoder_layer(x)\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f61e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, frame_encoding_size)\n",
    "        self.positional_encoding = PositionalEncoding(frame_encoding_size)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(\n",
    "                            d_model = frame_encoding_size,\n",
    "                            nhead = 8,\n",
    "                            dim_feedforward = frame_encoding_size * 2,\n",
    "                            batch_first = True\n",
    "                        )\n",
    "        self.linear_final = nn.Linear(frame_encoding_size, vocab_size)\n",
    "        \n",
    "    def forward(self, encoder_output, tgt, tgt_mask):\n",
    "        x = self.word_embedding(tgt)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.decoder_layer(x, encoder_output, tgt_mask)\n",
    "        return self.linear_final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b5eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "t5.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31704914",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b6df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96932f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(t5_tokenizer)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_to_enc = nn.Linear(1024, 512)\n",
    "enc_to_gloss_probs = nn.Sequential(\n",
    "    nn.Linear(512, vocab_size + 1),\n",
    "    nn.LogSoftmax(dim = -1)\n",
    ").to(device)\n",
    "ctc_loss = nn.CTCLoss(blank=vocab_size, zero_infinity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03034a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_opt = torch.optim.AdamW([\n",
    "    {'params': t5.parameters(), 'lr': 1e-4},\n",
    "    {'params': enc_to_gloss_probs.parameters(), 'lr': 5e-3},\n",
    "    {'params': frame_to_enc.parameters(), 'lr': 5e-3}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90587cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_embeds = frame_to_enc(test_batch[0].view(-1, 1024)).view(vid_batch_size, 200, 512).to(device)\n",
    "gloss_ids = t5_tokenizer(list(test_batch[1]), return_tensors='pt', padding='max_length', max_length=200).input_ids.to(device)\n",
    "translations_ids = t5_tokenizer(list(test_batch[2]), return_tensors='pt', padding='max_length', max_length=200).input_ids.to(device)\n",
    "outputs = t5(inputs_embeds=frame_embeds, labels=translations_ids)\n",
    "print(outputs.loss)\n",
    "# outputs.loss.backward()\n",
    "# t5_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c840c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.encoder_last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275bc4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_tokenizer.decode(0), t5_tokenizer.decode(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "gloss_probs = enc_to_gloss_probs(outputs.encoder_last_hidden_state.view(-1, 512)).view(vid_batch_size, 200, vocab_size).permute(1, 0, 2)\n",
    "input_lengths = torch.full(size=(vid_batch_size,), fill_value=200, dtype=torch.long).to(device)\n",
    "glosses = t5_tokenizer(list(batch[1]), return_tensors='pt', padding='max_length', max_length=200)\n",
    "gloss_ids = glosses.input_ids.to(device)\n",
    "gloss_lengths = torch.sum(glosses.attention_mask, dim = -1).to(device)\n",
    "recog_loss = ctc_loss(gloss_probs, gloss_ids, input_lengths, gloss_lengths)\n",
    "print(recog_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af195b9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "configuration = AutoConfig.from_pretrained('t5-small')\n",
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration.dropout_rate = 0.3\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\", config=configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8810a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab6b81cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "vocab_size = len(t5_tokenizer)\n",
    "ctc_loss = nn.CTCLoss(blank=vocab_size, zero_infinity=True)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e290d44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_and_opt(config):\n",
    "    hf_config = AutoConfig.from_pretrained('t5-small')\n",
    "    \n",
    "    if \"dropout_rate\" in config.keys():\n",
    "        hf_config.dropout_rate = config['dropout_rate']\n",
    "    \n",
    "    if \"layer_norm_epsilon\" in config.keys():\n",
    "        hf_config.layer_norm_epsilon = config['layer_norm_epsilon']\n",
    "    \n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\", config=hf_config).to(device)\n",
    "    \n",
    "    frame_to_enc = nn.Linear(1024, 512).to(device)\n",
    "    enc_to_gloss_probs = nn.Sequential(\n",
    "        nn.Linear(512, vocab_size + 1),\n",
    "        nn.LogSoftmax(dim = -1)\n",
    "    ).to(device)\n",
    "    \n",
    "    extra_layers = {\n",
    "        'frame_to_enc': frame_to_enc,\n",
    "        'enc_to_gloss_probs': enc_to_gloss_probs,\n",
    "    }\n",
    "    opt = torch.optim.AdamW([\n",
    "        {'params': model.parameters(), 'lr': config['model-lr']},\n",
    "        {'params': enc_to_gloss_probs.parameters(), 'lr': config['extra-layer-lr']},\n",
    "        {'params': frame_to_enc.parameters(), 'lr': config['extra-layer-lr']}\n",
    "    ])\n",
    "    \n",
    "    return model, extra_layers, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_tokenizer.decode(t5.generate(input_ids=input_ids)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "913e5842",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = load_metric(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "252cabba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, frame_to_enc, data_loader, metric):\n",
    "    progress = tqdm(range(len(data_loader)))\n",
    "    model.eval()\n",
    "    for s, g, t in data_loader:\n",
    "        s = s.to(device)\n",
    "        frame_embeds = frame_to_enc(s.view(-1, 1024)).view(len(s), 200, 512)\n",
    "        translations_ids = t5_tokenizer(list(t), return_tensors='pt', padding='max_length', max_length=200).input_ids.to(device)\n",
    "        outputs = model.generate(inputs_embeds=frame_embeds)\n",
    "        \n",
    "        predictions = [s.split(\" \") for s in t5_tokenizer.batch_decode(outputs, skip_special_tokens=True)]\n",
    "        references = [[s.split(\" \")] for s in t]\n",
    "        metric.add_batch(predictions=predictions, references=references)\n",
    "        \n",
    "        progress.update(1)\n",
    "    \n",
    "    return metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "419abd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, extra_layers, optimizer, tokenizer, recog_loss_weight, dataloaders, n_epochs, save_folder, eval_every=300, should_print=True):\n",
    "    trans_loss_history = {}\n",
    "    recog_loss_history = {}\n",
    "    total_loss_history = {}\n",
    "    train_bleu_history = {}\n",
    "    val_bleu_history = {}\n",
    "    test_bleu_history = {}\n",
    "    \n",
    "    train_loader, train_small_loader, val_loader, test_loader = dataloaders['train'], dataloaders['train-small'], dataloaders['val'], dataloaders['test']\n",
    "    frame_to_enc = extra_layers['frame_to_enc']\n",
    "    enc_to_gloss_probs = extra_layers['enc_to_gloss_probs']\n",
    "    \n",
    "    for epoch in tqdm(range(n_epochs), desc = 'Epochs'):\n",
    "        train_progress = tqdm(range(len(train_loader)), desc = 'Batches processed')\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            sign, gloss, text = batch\n",
    "            sign = sign.to(device)\n",
    "            frame_embeds = frame_to_enc(sign.view(-1, 1024)).view(len(sign), 200, 512)\n",
    "            translations_ids = tokenizer(list(text), return_tensors='pt', padding='max_length', max_length=200).input_ids.to(device)\n",
    "            outputs = model(inputs_embeds=frame_embeds, labels=translations_ids)\n",
    "\n",
    "            gloss_probs = enc_to_gloss_probs(outputs.encoder_last_hidden_state.view(-1, 512)).view(vid_batch_size, 200, vocab_size+1).permute(1, 0, 2)\n",
    "            input_lengths = torch.full(size=(vid_batch_size,), fill_value=200, dtype=torch.long).to(device)\n",
    "            glosses = tokenizer(list(gloss), return_tensors='pt', padding='max_length', max_length=200)\n",
    "            gloss_ids = glosses.input_ids.to(device)\n",
    "            gloss_lengths = torch.sum(glosses.attention_mask, dim = -1).to(device)\n",
    "            recog_loss = ctc_loss(gloss_probs, gloss_ids, input_lengths, gloss_lengths)\n",
    "\n",
    "            loss = (1 - recog_loss_weight) * outputs.loss + recog_loss_weight * recog_loss\n",
    "    #        loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % eval_every == 0:\n",
    "                model.eval()\n",
    "                trans_loss_history[str(epoch) + '-' + str(i)] = outputs.loss.item()\n",
    "                recog_loss_history[str(epoch) + '-' + str(i)] = recog_loss.item()\n",
    "                total_loss_history[str(epoch) + '-' + str(i)] = loss.item()\n",
    "                \n",
    "                train_bleu_history[str(epoch) + '-' + str(i)] = evaluate(model, frame_to_enc, train_small_loader, bleu)\n",
    "                val_bleu_history[str(epoch) + '-' + str(i)] = evaluate(model, frame_to_enc, val_loader, bleu)\n",
    "                test_bleu_history[str(epoch) + '-' + str(i)] = evaluate(model, frame_to_enc, test_loader, bleu)\n",
    "                model.save_pretrained(os.path.join(save_folder, str(epoch) + '-' + str(i)))\n",
    "\n",
    "                if should_print:\n",
    "                    print(\"Epoch:\", epoch, \"Iteration:\", i)\n",
    "                    print(\"Train bleu: {:.2%}\".format(train_bleu_history[str(epoch) + '-' + str(i)]['bleu']))\n",
    "                    print(\"Val bleu: {:.2%}\".format(val_bleu_history[str(epoch) + '-' + str(i)]['bleu']))\n",
    "                    print(\"Test bleu: {:.2%}\".format(test_bleu_history[str(epoch) + '-' + str(i)]['bleu']))\n",
    "                    print(\"Translation loss: {:.3f}\".format(outputs.loss.item()))\n",
    "                    print(\"Recognition loss: {:.3f}\".format(recog_loss.item()))\n",
    "                    print(\"\\n\")\n",
    "                    print(\"Gloss:\", gloss[0])\n",
    "                    print(\"Translation:\", text[0])\n",
    "                    print(\"Predicted translation:\", tokenizer.decode(model.generate(inputs_embeds=frame_embeds)[0]))\n",
    "                    print(\"========================================================================================\")\n",
    "                model.train()\n",
    "\n",
    "            train_progress.update(1)\n",
    "    \n",
    "    histories = {\n",
    "        'trans_loss': trans_loss_history,\n",
    "        'recog_loss': recog_loss_history,\n",
    "        'total_loss': total_loss_history,\n",
    "        'train_bleu': train_bleu_history,\n",
    "        'val_bleu': val_bleu_history,\n",
    "        'test_bleu': test_bleu_history,\n",
    "    }\n",
    "    return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "754ae1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    1: {\n",
    "        'recog_loss_weight': 0.01,\n",
    "        'dropout_rate': 0.1,\n",
    "        'model-lr': 1e-4,\n",
    "        'extra-layer-lr': 5e-3\n",
    "    },\n",
    "    \n",
    "    2: {\n",
    "        'recog_loss_weight': 0.1,\n",
    "        'dropout_rate': 0.1,\n",
    "        'model-lr': 1e-4,\n",
    "        'extra-layer-lr': 5e-3\n",
    "    },\n",
    "    \n",
    "    3: {\n",
    "        'recog_loss_weight': 1.0,\n",
    "        'dropout_rate': 0.1,\n",
    "        'model-lr': 1e-4,\n",
    "        'extra-layer-lr': 5e-3\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8089f96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_histories = []\n",
    "for i, config in configs.items():\n",
    "    \n",
    "    print(\"Initializing model...\")\n",
    "    model, extra_layers, opt = initialize_model_and_opt(config)\n",
    "    print(\"Done! Training...\")\n",
    "    histories = train(\n",
    "        model = model,\n",
    "        extra_layers = extra_layers,\n",
    "        optimizer = opt,\n",
    "        tokenizer = t5_tokenizer,\n",
    "        recog_loss_weight = config['recog_loss_weight'],\n",
    "        dataloaders = {\n",
    "            'train': phoenix_train_loader,\n",
    "            'train-small': phonix_train_small_loader,\n",
    "            'val': phoenix_val_loader,\n",
    "            'test': phoenix_test_loader\n",
    "        },\n",
    "        n_epochs = 10,\n",
    "        save_folder = \"Models/Phoenix14T/\" + str(i)\n",
    "    )\n",
    "    \n",
    "    del model\n",
    "    del extra_layers\n",
    "    del opt\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    all_histories.append(histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31deda08",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(t5, phoenix_train_loader, bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9519cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_encoder = TransformerEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_encoder(test_batch[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce41be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_decoder = TransformerDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b67d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_decoder.decoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c27f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_tokenizer.pad_token = german_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aa81de",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_encoder.to(device)\n",
    "t_decoder.to(device)\n",
    "optimizer = torch.optim.AdamW([{'params': t_encoder.parameters()}, {'params': t_decoder.parameters()}], lr=1e-3)\n",
    "out_to_gloss_probs = nn.Sequential(\n",
    "    nn.Linear(frame_encoding_size, vocab_size+1),\n",
    "    nn.LogSoftmax(dim = -1)\n",
    ").to(device)\n",
    "ctc_loss = nn.CTCLoss(zero_infinity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43711aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f83aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "inputs = batch[0].to(device)\n",
    "out = bert(inputs_embeds=inputs).last_hidden_state\n",
    "print(out.shape)\n",
    "gloss_probs = out_to_gloss_probs(out.view(-1, frame_encoding_size)).view(vid_batch_size, slrt_input_len, vocab_size+1).permute(1, 0, 2)\n",
    "print(gloss_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4531c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.argmax(gloss_probs, dim = -1)\n",
    "print(german_tokenizer.decode(pred[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch[2][0])\n",
    "target = german_tokenizer(list(batch[2]), return_tensors='pt', padding='max_length', max_length=max_vid_len)\n",
    "target.input_ids, target.input_ids.shape, torch.sum(target.attention_mask, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths = torch.full(size=(vid_batch_size,), fill_value=slrt_input_len, dtype=torch.long)\n",
    "loss = ctc_loss(gloss_probs, target.input_ids, input_lengths, torch.sum(target.attention_mask, dim = -1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4286c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f66a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0fbe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db605a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(phoenix_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d271c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259101e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bos = german_tokenizer.bos_token\n",
    "bos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e3a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = german_tokenizer([bos + t for t in test_batch[2]], return_tensors='pt', padding='max_length', max_length=200)\n",
    "decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f5b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "look_ahead_mask = torch.triu(torch.ones(200, 200) * float('-inf'), diagonal=1)\n",
    "look_ahead_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab23549",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=-1)\n",
    "ce_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47b681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory = test_batch[0].to(device)\n",
    "tgt = decoder_input.input_ids.to(device)\n",
    "look_ahead_mask = look_ahead_mask.to(device)\n",
    "outs = t_decoder(memory, tgt, look_ahead_mask)\n",
    "probs = softmax(outs)\n",
    "targets = german_tokenizer(list(test_batch[2]), return_tensors='pt', padding='max_length', max_length=200).input_ids.to(device)\n",
    "loss = ce_loss(outs.view(-1, vocab_size), targets.view(-1))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_tokenizer.batch_decode(torch.argmax(probs, dim=-1))[0], test_batch[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b35fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a2537",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_progress = tqdm(phoenix_train)\n",
    "for i, batch in enumerate(phoenix_train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    inputs = batch[0].to(device)\n",
    "    out = t_encoder(inputs)\n",
    "    #print(out.shape)\n",
    "    gloss_probs = out_to_gloss_probs(out.view(-1, frame_encoding_size)).view(vid_batch_size, slrt_input_len, vocab_size+1).permute(1, 0, 2)\n",
    "    #print(gloss_probs.shape)\n",
    "    target = german_tokenizer(list(batch[1]), return_tensors='pt', padding='max_length', max_length=100).to(device)\n",
    "    if 0 in target:\n",
    "        print(\"BREAKING\")\n",
    "        break\n",
    "    input_lengths = torch.full(size=(vid_batch_size,), fill_value=slrt_input_len, dtype=torch.long)\n",
    "    recog_loss = ctc_loss(gloss_probs, target.input_ids, input_lengths, torch.sum(target.attention_mask, dim = -1))\n",
    "    \n",
    "    \n",
    "    print(i, recog_loss)\n",
    "    if i % 100 == 0:\n",
    "        pred = torch.argmax(gloss_probs, dim = -1)\n",
    "        print(german_tokenizer.decode(pred[:, 0]))\n",
    "        print(batch[2][0])\n",
    "    train_progress.update(len(batch[0]))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c485fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_tokenizer.encode(\"NORDWEST WIND SCHWACH MAESSIG SUED SCHWACH BEWEGEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d4738",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_tokenizer.decode([50, 19187, 59, 12044, 336])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1622abf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0616874d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "F.softmax(gloss_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e1558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "out = bert(vid_encoder_input.cpu()).last_hidden_state\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431faa8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gloss_probs = out_to_gloss_probs(out.view(-1, frame_encoding_size)).view(vid_batch_size, slrt_input_len, vocab_size).permute(1, 0, 2)\n",
    "gloss_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffd2ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[2]['gloss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_tokenizer([batch[2]['gloss'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d645681",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = german_tokenizer(batch[2]['gloss'], return_tensors='pt', padding='max_length', max_length=max_vid_len)\n",
    "target.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f9162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths = torch.full(size=(vid_batch_size,), fill_value=slrt_input_len, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a8720",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69898150",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths, batch[0], gloss_probs.shape, target.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c9150",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = ctc_loss(gloss_probs, target.input_ids, input_lengths, torch.sum(target.attention_mask, dim = -1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7725d67d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.utils.clip_grad_norm_(bert.parameters(), max_norm=max_gradient_norm, error_if_nonfinite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152dc904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target are to be padded\n",
    "T = 50      # Input sequence length\n",
    "C = 20      # Number of classes (including blank)\n",
    "N = 16      # Batch size\n",
    "S = 30      # Target sequence length of longest target in batch (padding length)\n",
    "S_min = 10  # Minimum target length, for demonstration purposes\n",
    "\n",
    "# Initialize random batch of input vectors, for *size = (T,N,C)\n",
    "input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
    "# input = gloss_probs\n",
    "#print(torch.sum(input, dim=-1))\n",
    "# Initialize random batch of targets (0 = blank, 1:C = classes)\n",
    "target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)\n",
    "\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths, target_lengths, input.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c02cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoEncoder(nn.Module):\n",
    "    def __init__(self, vid_dim, d_model, n_head, n_layers):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, n_head, batch_first=True)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, n_layers)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        src = src * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.encoder(src, src_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e96ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
